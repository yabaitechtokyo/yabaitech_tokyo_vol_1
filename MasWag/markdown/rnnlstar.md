# RNNによる等価性質問

前節では所属性質問と等価性質問に答える神託が存在する、という仮定の元で正規言語を学習する、L\*アルゴリズムについて説明しました。本節ではこのような神託をRNNを用いて近似的に実現する手法について説明します。本節の内容が \\cite([\`conf/icml/WeissGY18\`]); の主な貢献となります。本節での目標は、正規言語$L$の受理/非受理についての二値分類問題を正しく解くRNN $R$が与えられた際に、前節で考えていた所属性質問と等価性質問に答える、というものになります。このような問題を既存のRNNの学習手法及び前節で説明したL\*アルゴリズムと組み合わせることにより、例えばノイズの乗った訓練用データから正規言語を学習することができます。

本節では、RNNの状態空間を$\mathbb{R}^N$、RNNの初期状態を$h_0\in\mathbb{R}^N$、RNNの状態$h\in\mathbb{R}^N$に文字$a\in\Sigma$ないし文字列$w\in\Sigma^\ast$を与えたときの状態ベクトルを$g(h,a)$や$g(h,w)$、RNNの状態$h\in\mathbb{R}^N$に対して受理、非受理の二値を割り当てる関数を$f\colon\mathbb{R}^N\to\{\mathrm{T},\mathrm{F}\}$と表記します。つまり、文字列$w\in\Sigma^\ast$について、$f(g(h_0,w))=\mathrm{T}$であるとき、このRNNは文字列$w$を受理すると分類した、ということになります。

さて、まず所属性質問についてRNNを用いて答えることを考えますが、これはとても簡単です。というのも上記の様に、文字列$w\in\Sigma^\ast$について、$f(g(h_0,w))$の値を見ることで$w$が学習したい言語$L$に所属しているかどうかを調べることができるからです。従って以下ではRNNを用いて等価性質問に如何に答えるかということに注目していきます。

## RNNによる等価性質問の概要

等価性質問について、\\cite([\`conf/icml/WeissGY18\`]);ではRNNからある種のオートマトンを構成しながら答えていきます。まず、状態数が無限状態で構わなければ、状態空間を$\mathbb{R}^N$、初期状態を$h_0$、受理状態を$f^{-1}(\mathrm{T})$、遷移関数を$\Delta(h,a)=g(h,a)$とすることで、RNNを無限状態のオートマトンと見ることができます(図の上側)。しかし、等価性質問に答えるためには状態数が有限である必要があります。そこで、Myhill-Nerode同値関係のように、同じ状態と見なしても構わない状態を一つのクラスタとして纏める (クラスタリング) ことでDFAを構築します (図の下側)。つまりクラスタリングの関数$p:\mathbb{R}^N\to\mathbb{N}$で、以下の性質が成り立ち、像が有限集合となるようなものを求めることができれば、$p$の像を状態空間とするDFAを構築することができます。

* RNNの状態$h,h'\in\mathbb{R}^N$について$p(h)=p(h')$ならば任意の文字$a\in\Sigma$について$p(g(h,a))=p(g(h',a))$
* RNNの状態$h,h'\in\mathbb{R}^N$について$p(h)=p(h')$ならば$f(h)=f(h')$

ところで、このクラスタリングの関数$p$を正確に求めることができるのであればそもそもRNNからDFAを抽出できているので、DFAが正確に学習できているとも言えますが、それは非常に難しいです。例えば矛盾が起こらない様にRNNの全ての状態について幅優先探索によって調べれば正確なクラスタリングを行うことができますが有限時間では行えません。従ってこのクラスタリングについては途中で諦める必要があります。この途中で諦めるという点について、Myhill-Nerodeの定理などを使いつつ、程良く諦める手法を提案しているというのが\\cite([\`conf/icml/WeissGY18\`]);の技術的な肝になります。以下ではRNNによる等価性質問の具体的な流れについて説明をします。

## 探索のための初期化

先に説明した様に\\cite([\`conf/icml/WeissGY18\`]);では幅優先探索によってクラスタリングを求めます。おおまかな流れは、入力文字列について探索することによって、今のクラスタリングにおける反例を見付け、それに基いてクラスタリングを更新する、という操作をクラスタリングが十分正確になるまで繰り返すことで、十分正確なクラスタリングを得るというものになります。
クラスタリング$p$は任意の$h\in\mathbb{R}^N$について$p(h)=0$となるように、次に調べる文字列のキュー $W_{\mathrm{new}}$に$\varepsilon$をpushした状態で初期化し、訪問済の文字列の集合 $W_{\mathrm{seen}}\in\mathcal{P}(\mathbb{N})$及びクラスタの集合$P_{\mathrm{seen}}\in\mathcal{P}(\mathbb{N})$ は 共に$\emptyset$で初期化します。


## Step 1: 矛盾かどうかの確認

まず始めに次に調べる文字列のキュー$W_{\mathrm{new}}$から文字列$w$をpopします。ここで既にキュー$W_{\mathrm{new}}$が空であった場合、入力のRNN$R$とDFA$\mathcal{A}$は十分似ているということで、等価であるという返答を返します。

文字列$w$について、まずRNN$R$における受理/非受理とDFA$\mathcal{A}$における受理/非受理が一致するかを調べて、一致しない場合は$w$を反例として返します。これによって、既に調べた文字列については$R$と$\mathcal{A}$が同じ分類をするということを保証できます。次に、今まで調べた文字列$w'$で、$p(g(h_0,w))$と$p(g(h_0,w'))$が一致するものについて、DFA$\mathcal{A}$上の状態$\Delta(q_0,w)$と$\Delta(q_0,w')$が一致するかどうかを調べます。これらが一致する場合、クラスタリングに矛盾はないのでStep 2-1に移ります。これらが一致しない場合クラスタリングに問題があるか$R$と$\mathcal{A}$が一致しないということになるので、Step 2-2でより詳しく見ていきます。

## Step 2-1: 新たな点の追加

DFA$\mathcal{A}$上の状態$\Delta(q_0,w)$と$\Delta(q_0,w')$が一致する場合、まず$w$を訪問済の文字列の集合$W_{\mathrm{seen}}$に追加します。$p(w)$が既に訪問済である (つまり$p(w)\in P_{\mathrm{seen}}$)場合はそのままStep 1に戻りますが、$p(w)$に未だ訪問したことがない場合、$w$に一文字加えた文字列についても探索を行います。つまり、$p(w)$を$P_{\mathrm{seen}}$に追加して、各$a\in\Sigma$について$w\cdot a$を$W_{\mathrm{new}}$にpushして、Step 1に戻ります。

## Step 2-2: クラスタリングの改良

DFA$\mathcal{A}$上の状態$\Delta(q_0,w)$と$\Delta(q_0,w')$が一致しない場合、この不一致の原因がクラスタリングにあるのか、そもそもRNN$R$とDFA$\mathcal{A}$の表している言語が異なるのかを調べる必要があります。そのために、まず$\Delta(q_0,w\cdot w'')$と$\Delta(q_0,w'\cdot w'')$が異なる様な文字列$w''\in\Sigma^\ast$を見付けます。このような文字列はL\*アルゴリズムによって得られるDFAの最小性から必ず存在します。

次に、各$p(g(h_0,w))=p(g(h_0,u))$を充たすような訪問済の文字列$u\in W_{\mathrm{seen}}$について、RNN$R$とDFA$\mathcal{A}$において$u\cdot w''$の受理/非受理が一致しないものが存在するかどうかを調べます。もし受理/非受理が一致しないものが存在するのであれば、それが反例となるので、$R$と$\mathcal{A}$が異なる言語を認識するという証拠として$u\cdot w''$を返します。もし受理/非受理が常に一致するのであれば、本来$p(g(h_0,w))\neq p(g(h_0,w'))$であるべきであるにもかかわらず、クラスタリングが粗すぎるために$p(g(h_0,w))= p(g(h_0,w'))$であると言うことができます。そのため、SVMなどによってクラスタリングを改良して、Step 1に戻ります。このとき、今迄訪問した点についての情報を引き継ぐことができないので、$W_{\mathrm{new}}$、$W_{\mathrm{seen}}\in\mathcal{P}(\mathbb{N})$、$P_{\mathrm{seen}}\in\mathcal{P}(\mathbb{N})$は全て初期化します。

